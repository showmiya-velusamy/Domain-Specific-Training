{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtUZKwPIDmrAwe8MIhaej/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/showmiya-velusamy/Domain-Specific-Training/blob/main/ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IcVfkUegEoTn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "facd4aac-372f-424b-a36d-daa5938314bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=c11e61b31355c6dc29fb87d7170101b35f49bd0c922a04909561615c3fc7e90b\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#assignment 1\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DeltaLakeAssignment\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Import Delta Lake packages\n",
        "from delta.tables import *\n",
        "\n",
        "#Task 1: Creating Delta Table using Three Methods\n",
        "# Load the CSV dataset (Employees)\n",
        "employees_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/path_to/employees.csv\")\n",
        "\n",
        "# Load the JSON dataset (Products)\n",
        "products_df = spark.read.format(\"json\").load(\"/path_to/products.json\")\n",
        "\n",
        "# Write the DataFrame as a Delta Table (Employees)\n",
        "employees_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path_to/delta/employees_delta\")\n",
        "\n",
        "# Write the DataFrame as a Delta Table (Products)\n",
        "products_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path_to/delta/products_delta\")\n",
        "\n",
        "# Create Delta table for Employees using SQL\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE delta_employees\n",
        "    USING DELTA\n",
        "    AS SELECT * FROM parquet./path_to/delta/employees_delta\n",
        "\"\"\")\n",
        "\n",
        "# Create Delta table for Products using SQL\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE delta_products\n",
        "    USING DELTA\n",
        "    AS SELECT * FROM parquet./path_to/delta/products_delta\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Task 2: Merge and Upsert (Slowly Changing Dimension - SCD)\n",
        "#Load the Delta table for employees created in Task 1.\n",
        "\n",
        "employee_df_updates = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/employee_updates.csv\")\n",
        "employee_df_updates.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_updates\")\n",
        "\n",
        "employee_df_updates = spark.read.format(\"delta\").load(\"/delta/employee_updates\")\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE delta_employee_updates\n",
        "    USING DELTA\n",
        "    AS SELECT * FROM parquet./path_to/delta/employees_updates\n",
        "\"\"\")\n",
        "\n",
        "#Merge the new employee data into the employees Delta table.\n",
        "spark.sql(\"\"\"\n",
        "    MERGE INTO delta_employees AS target\n",
        "    USING employee_updates AS source\n",
        "    ON target.EmployeeID = source.EmployeeID\n",
        "    WHEN MATCHED THEN UPDATE SET target.Salary = source.Salary, target.Department = source.Department\n",
        "    WHEN NOT MATCHED THEN INSERT (EmployeeID, Name, Department, JoiningDate, Salary)\n",
        "    VALUES (source.EmployeeID, source.Name, source.Department, source.JoiningDate, source.Salary)\n",
        "\"\"\")\n",
        "\n",
        "#Task 3: Internals of Delta Table\n",
        "#Check the transaction history of the table.\n",
        "spark.sql(\"DESCRIBE HISTORY delta_employee\").show(truncate=False)\n",
        "\n",
        "#Perform Time Travel and retrieve the table before the previous merge operation.\n",
        "\n",
        "version_before_merge = delta_employee_table.history().filter(\"operation = 'MERGE'\").select(\"version\").first()[0] - 1\n",
        "\n",
        "# Time Travel: Retrieve data before the previous merge\n",
        "previous_version_df = spark.read.format(\"delta\").option(\"versionAsOf\", version_before_merge).load(\"/path_to/delta/employees_delta\")\n",
        "previous_version_df.show()\n",
        "\n",
        "\n",
        "#Task 4: Optimize Delta Table\n",
        "#Optimize the employees Delta table for better performance.\n",
        "spark.sql(\"OPTIMIZE delta_employee_table\")\n",
        "\n",
        "#Use Z-ordering on the Department column for improved query performance.\n",
        "spark.sql(\"\"\"\n",
        "OPTIMIZE delta_employee_table ZORDER BY Department \"\"\")\n",
        "\n",
        "#Task 5: Time Travel with Delta Table\n",
        "#Retrieve the employees Delta table as it was before the last merge.\n",
        "time_travel_df = spark.read.format(\"delta\").option(\"versionAsOf\", version_before_merge).load(\"/path_to/delta/employees_delta\")\n",
        "time_travel_df.show()\n",
        "\n",
        "#Task 6: Vacuum Delta Table\n",
        "#Use the vacuum operation on the employees Delta table to remove old versions and free up disk space.\n",
        "#Set the retention period to 7 days and ensure that old files are deleted.\n",
        "spark.sql(\"\"\"\n",
        "VACUUM delta_employee_table RETAIN 168 HOURS \"\"\")"
      ],
      "metadata": {
        "id": "W2RAsldxVQcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ASSIGNMENT 2\n",
        "\n",
        "dbutils.fs.cp(\"file:/content/sample_data/sales_data.csv\", \"dbfs:/FileStore/streaming/input/sales_data.csv\")\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#Initialize SparkSession\n",
        "\n",
        "spark =SparkSession.builder\\\n",
        "     .appName(\"StructuredStreamingExample\") \\\n",
        "     .getOrCreate()\n",
        "\n",
        "#TASK 1\n",
        "#Define the schema for the CSV data\n",
        "sales_schema = \"TransactionID INT, TransactionDate STRING, ProductID INT, Quantity INT, Price DOUBLE\"\n",
        "\n",
        "#Read streaming data from CSV files\n",
        "df_sales_stream =spark.readStream\\\n",
        "          .format(\"csv\") \\\n",
        "          .option(\"header\", \"true\") \\\n",
        "          .schema (sales_schema)\\\n",
        "          .load(\"dbfs:/Filestore/streaming/input/\")\n",
        "\n",
        "query = spark.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "query.awaitTermination()\n",
        "\n",
        "#TASK 2\n",
        "transformed_df = spark.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n",
        "                   .filter(col(\"Quantity\") > 1)\n",
        "\n",
        "memory_query = transformed_df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"transformed_data\") \\\n",
        "    .start()\n",
        "\n",
        "memory_query.awaitTermination()\n",
        "spark.sql(\"SELECT * FROM transformed_data\").show()\n",
        "\n",
        "#TASK 3\n",
        "aggregated_df = transformed_df.groupBy(\"ProductID\") \\\n",
        "                              .agg({\"TotalAmount\": \"sum\"}) \\\n",
        "                              .withColumnRenamed(\"sum(TotalAmount)\", \"TotalSales\")\n",
        "\n",
        "aggregated_query = aggregated_df.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "aggregated_query.awaitTermination()\n",
        "\n",
        "#TASK 4\n",
        "query_to_parquet = (aggregated_query.writeStream\n",
        "                                .format(\"parquet\")\n",
        "                                .outputMode(\"append\")\n",
        "                                .option(\"path\", \"/output/stream_parquet/\")\n",
        "                                .option(\"checkpointLocation\", \"/output/checkpoints/\")\n",
        "                                .start())\n",
        "\n",
        "#TASK 5\n",
        "watermark_df = transformed_df.withWatermark(\"TransactionDate\", \"1 day\")\n",
        "watermarked_aggregated_df = watermark_df.groupBy(\"ProductID\") \\\n",
        "                                        .agg({\"TotalAmount\": \"sum\"}) \\\n",
        "                                        .withColumnRenamed(\"sum(TotalAmount)\", \"TotalSales\")\n",
        "\n",
        "#TASK 6\n",
        "product_schema = StructType([\n",
        "    StructField(\"ProductID\", StringType(), True),\n",
        "    StructField(\"ProductName\", StringType(), True),\n",
        "    StructField(\"Category\", StringType(), True)\n",
        "])\n",
        "\n",
        "products_df = spark.readStream \\\n",
        "    .schema(product_schema) \\\n",
        "    .csv(\"file:/content/sample_data/sales_data.csv\")\n",
        "\n",
        "joined_df = transformed_df.join(products_df, on=\"ProductID\", how=\"inner\")\n",
        "\n",
        "joined_query = joined_df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "joined_query.awaitTermination()\n",
        "\n",
        "#TASK 7\n",
        "query.stop()\n",
        "query_restarted = (df_sales_stream.writeStream\n",
        "                               .format(\"parquet\")\n",
        "                               .outputMode(\"append\")\n",
        "                               .option(\"path\", \"/output/stream_restarted/\")\n",
        "                               .option(\"checkpointLocation\", \"/output/checkpoints/\")\n",
        "                               .start())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "Vy-G1SuYIINz",
        "outputId": "6fa51ee5-3838-42a2-b196-03df11d5c912"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dbutils' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-37d7670d12a0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:/content/sample_data/sales_data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dbfs:/FileStore/streaming/input/sales_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dbutils' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ASSIGNMENT 3\n",
        "\n",
        "# Task 1: Create an ETL Pipeline using DLT (Python)\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Read the source data from CSV/Parquet\n",
        "source_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:/content/sample_data/orders_data.csv\")\n",
        "\n",
        "# Transform the data\n",
        "# Add a new column 'TotalAmount' by multiplying 'Quantity' by 'Price'\n",
        "transformed_df = source_df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\"))\n",
        "\n",
        "# Filter where Quantity is greater than 1\n",
        "transformed_filtered_df = transformed_df.filter(col(\"Quantity\") > 1)\n",
        "\n",
        "# Write the transformed data to a Delta table\n",
        "transformed_filtered_df.write.format(\"delta\").mode(\"overwrite\").save(\"file:/content/sample_data/delta_orders_table\")\n",
        "\n",
        "# Task 2: Create an ETL Pipeline using DLT (SQL)\n",
        "# Create a table from the source CSV data\n",
        "spark.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE orders_raw\n",
        "USING CSV\n",
        "OPTIONS (path \"file:/content/sample_data/orders_data.csv\", header \"true\");\n",
        "\"\"\")\n",
        "\n",
        "# 2: Transform the data by adding TotalAmount and filtering\n",
        "spark.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE orders_transformed AS\n",
        "SELECT *, (Quantity * Price) AS TotalAmount\n",
        "FROM orders_raw\n",
        "WHERE Quantity > 1;\n",
        "\"\"\")\n",
        "\n",
        "# 3: Write the transformed data into a Delta table\n",
        "spark.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE delta_orders_table AS\n",
        "SELECT * FROM orders_transformed;\n",
        "\"\"\")\n",
        "\n",
        "# Task 3: Perform Read, Write, Update, and Delete Operations on Delta Table\n",
        "# 1. Reading the data from the Delta table (PySpark)\n",
        "df = spark.read.format(\"delta\").load(\"file:/content/sample_data/delta_orders_table\")\n",
        "df.show()\n",
        "\n",
        "# 2. Update the table (Increase the price of laptops by 10%)\n",
        "spark.sql(\"\"\"\n",
        "    UPDATE delta_orders_table\n",
        "    SET Price = Price * 1.10\n",
        "    WHERE Product = 'Laptop'\n",
        "\"\"\")\n",
        "\n",
        "# 3. Delete rows where quantity is less than 2\n",
        "spark.sql(\"\"\"\n",
        "    DELETE FROM delta_orders_table\n",
        "    WHERE Quantity < 2\n",
        "\"\"\")\n",
        "\n",
        "# 4. Insert a new record\n",
        "spark.sql(\"\"\"\n",
        "    INSERT INTO delta_orders_table (OrderID, OrderDate, CustomerID, Product, Quantity, Price, TotalAmount)\n",
        "    VALUES (106, '2024-01-06', 'C006', 'Keyboard', 3, 50, 150)\n",
        "\"\"\")\n",
        "\n",
        "# Task 4: Merge Data (SCD Type 2)\n",
        "spark.sql(\"\"\"MERGE INTO delta_orders_table AS target\n",
        "USING (SELECT * FROM new_orders_data) AS source\n",
        "ON target.OrderID = source.OrderID\n",
        "WHEN MATCHED THEN\n",
        "  UPDATE SET target.Quantity = source.Quantity,\n",
        "             target.Price = source.Price,\n",
        "             target.TotalAmount = source.Quantity * source.Price\n",
        "WHEN NOT MATCHED THEN\n",
        "  INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price, TotalAmount)\n",
        "  VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price, source.Quantity * source.Price);\n",
        "\"\"\")\n",
        "\n",
        "# Task 5: Explore Delta Table Internals\n",
        "#View the transaction history of the Delta table\n",
        "spark.sql(\"\"\"\n",
        "DESCRIBE HISTORY delta_orders_table;\n",
        "\"\"\")\n",
        "\n",
        "#View file size and modification times\n",
        "spark.sql(\"\"\"\n",
        "DESCRIBE DETAIL delta_orders_table;\n",
        "\"\"\")\n",
        "\n",
        "# Task 6: Time Travel in Delta Tables\n",
        "#Query the table at a previous version\n",
        "df_time_travel = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
        "df_time_travel.show(truncate=False)\n",
        "\n",
        "#Query the table using a timestamp\n",
        "(\"\"\"\n",
        "SELECT * FROM delta_orders_table TIMESTAMP AS OF '2024-01-10 00:00:00';\n",
        "\"\"\")\n",
        "\n",
        "# Task 7: Optimize Delta Table\n",
        "#Optimize the Delta table using Z-order on the Product column\n",
        "spark.sql(\"\"\"OPTIMIZE delta_orders_table\n",
        "ZORDER BY (Product);\n",
        "\"\"\")\n",
        "\n",
        "#Vacuum the table to remove old files (older than 7 days)\n",
        "spark.sql(\"\"\"\n",
        "VACUUM delta_orders_table RETAIN 7 HOURS;\n",
        "\"\"\")\n",
        "\n",
        "# Task 8: Converting Parquet Files to Delta Format\n",
        "# Read the Parquet file\n",
        "parquet_df = spark.read.format(\"parquet\").load(\"file:/content/sample_data/historical_orders_parquet/\")\n",
        "\n",
        "# Convert the Parquet file to Delta\n",
        "parquet_df.write.format(\"delta\").save(\"file:/content/sample_data/historical_orders_delta/\")\n",
        "\n",
        "# Verify by querying the Delta table\n",
        "delta_df = spark.read.format(\"delta\").load(\"file:/content/sample_data/to/historical_orders_delta/\")\n",
        "delta_df.show()"
      ],
      "metadata": {
        "id": "hetaBNBwWDww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark\n",
        "pip install delta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "YHgm2HrnSkzK",
        "outputId": "c3789edb-814f-4678-fe0a-91ad8deeda2c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-6-49f37b97164a>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-49f37b97164a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install pyspark\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ASSIGNMENT 4\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from delta import *\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Read the CSV file\n",
        "df = spark.read.csv(\"file:/content/sample_data/orders_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Perform transformation\n",
        "transformed_df = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n",
        "                   .filter(col(\"Quantity\") > 5)\n",
        "\n",
        "# Write the transformed data to a Delta table\n",
        "transformed_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"transformed_orders\")\n",
        "\n",
        "# Read from the Delta table\n",
        "df = spark.table(\"transformed_orders\")\n",
        "\n",
        "# Perform aggregation\n",
        "aggregated_df = df.groupBy(\"Product\").agg({\"Quantity\": \"sum\"})\n",
        "\n",
        "# Write the aggregated data to a Delta table\n",
        "aggregated_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"aggregated_orders\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "JQtzJkjzTwlI",
        "outputId": "528963b3-0683-43f3-bc4c-d8c0c88b60a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'delta'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-65c423935706>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'delta'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}