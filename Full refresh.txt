import pandas as pd
from pyspark.sql import SparkSession

# Initialize Spark Session 
spark = SparkSession.builder.appName("CreateViewExample").getOrCreate()

# Full refresh: Load the entire dataset

df_sales = spark.read.format("csv") \
  .option("header", "true") \
  .option("inferSchema", "true") \
  .load("/content/sample_data/sales_data.csv")

# Apply transformations (if necessary) 
df_transformed = df_sales.withColumn("total_sales", df_sales["quantity"]* df_sales["price"])

# Full refresh: Partition the data by 'date' and overwrite the existing data 
output_path = "/content/sample_data/partitioned_data"
df_transformed.write.partitionBy("date").mode("overwrite").parquet(output_path)

# Verify partitioned data
partitioned_df = spark.read.parquet(output_path) 
partitioned_df.show()